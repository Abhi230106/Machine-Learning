{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FZJmxFo_Dsc",
        "outputId": "d77ee5d8-3231-4409-8d3d-23a1c3362d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh-ZVsfNakhI",
        "outputId": "cda44b96-58c2-45e1-903e-d2845a564a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Dataset  Index  Y_Actual  Y_Predicted  Absolute_Error\n",
            "0     Error Embeddings      0         8     6.651331        1.348669\n",
            "1     Error Embeddings      1         8     6.568666        1.431334\n",
            "2     Error Embeddings      2         8     6.840986        1.159014\n",
            "3     Error Embeddings      3         8     7.066110        0.933890\n",
            "4     Error Embeddings      4         8     7.428260        0.571740\n",
            "5   Chunked Embeddings      0         8     7.290345        0.709655\n",
            "6   Chunked Embeddings      1         8     7.120361        0.879639\n",
            "7   Chunked Embeddings      2         8     7.896743        0.103257\n",
            "8   Chunked Embeddings      3         8     6.974581        1.025419\n",
            "9   Chunked Embeddings      4         8     7.626843        0.373157\n",
            "10       Chunked Logic      0         8     7.336868        0.663132\n",
            "11       Chunked Logic      1         8     7.095735        0.904265\n",
            "12       Chunked Logic      2         8     7.866401        0.133599\n",
            "13       Chunked Logic      3         8     7.071730        0.928270\n",
            "14       Chunked Logic      4         8     7.657441        0.342559\n",
            "\n",
            "Summary (Lower Error is Better):\n",
            "              Dataset  Mean_Absolute_Error\n",
            "0  Chunked Embeddings             0.618225\n",
            "1       Chunked Logic             0.594365\n",
            "2    Error Embeddings             1.088929\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# File paths\n",
        "file_paths = {\n",
        "    \"error_embeddings\": \"avg_mark_code_with_error_embeddings.xlsx\",\n",
        "    \"chunked_embeddings\": \"avg_mark_chunked_code_embeddings.xlsx\",\n",
        "    \"chunked_logic\": \"avg_Chunked_Embedding_Python_code(logic).xlsx\"\n",
        "}\n",
        "\n",
        "# Function to process dataset with CatBoost\n",
        "def process_dataset(file_path, name):\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Define target and features\n",
        "    target = \"Average Total (Rounded)\"\n",
        "    y = df[target]\n",
        "    X = df.drop(columns=[target], errors='ignore').select_dtypes(include=np.number)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train = X.iloc[5:]\n",
        "    y_train = y.iloc[5:]\n",
        "    X_test = X.iloc[:5]\n",
        "    y_test = y.iloc[:5]\n",
        "\n",
        "    # Train CatBoost model\n",
        "    model = CatBoostRegressor(verbose=0, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Results\n",
        "    result = pd.DataFrame({\n",
        "        \"Dataset\": name,\n",
        "        \"Index\": X_test.index,\n",
        "        \"Y_Actual\": y_test.values,\n",
        "        \"Y_Predicted\": y_pred,\n",
        "        \"Absolute_Error\": np.abs(y_test.values - y_pred)\n",
        "    })\n",
        "\n",
        "    return result\n",
        "\n",
        "# Combine results from all datasets\n",
        "final_results = pd.concat([\n",
        "    process_dataset(file_paths[\"error_embeddings\"], \"Error Embeddings\"),\n",
        "    process_dataset(file_paths[\"chunked_embeddings\"], \"Chunked Embeddings\"),\n",
        "    process_dataset(file_paths[\"chunked_logic\"], \"Chunked Logic\")\n",
        "])\n",
        "\n",
        "# Reset index for clarity\n",
        "final_results.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Show results\n",
        "print(final_results)\n",
        "\n",
        "# Optional: Summary by dataset\n",
        "summary = final_results.groupby(\"Dataset\")[\"Absolute_Error\"].mean().reset_index()\n",
        "summary.columns = [\"Dataset\", \"Mean_Absolute_Error\"]\n",
        "print(\"\\nSummary (Lower Error is Better):\")\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# File paths\n",
        "file_paths = {\n",
        "    \"Error Embeddings\": \"avg_mark_code_with_error_embeddings.xlsx\",\n",
        "    \"Chunked Embeddings\": \"avg_mark_chunked_code_embeddings.xlsx\",\n",
        "    \"Chunked Logic\": \"avg_Chunked_Embedding_Python_code(logic).xlsx\"\n",
        "}\n",
        "\n",
        "# Store predictions and actuals by index\n",
        "prediction_dict = {}\n",
        "actual_marks = None\n",
        "\n",
        "# Process all datasets\n",
        "def train_and_predict(file_path, label):\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    y = df[\"Average Total (Rounded)\"]\n",
        "    X = df.drop(columns=[\"Average Total (Rounded)\"], errors='ignore').select_dtypes(include=np.number)\n",
        "\n",
        "    X_train = X.iloc[5:]\n",
        "    y_train = y.iloc[5:]\n",
        "    X_test = X.iloc[:5]\n",
        "    y_test = y.iloc[:5]\n",
        "\n",
        "    model = CatBoostRegressor(verbose=0, random_state=42)\n",
        "    param_grid = {\n",
        "        'depth': [4, 6],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'iterations': [50, 100]\n",
        "    }\n",
        "\n",
        "    grid = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    for idx, pred in zip(X_test.index, y_pred):\n",
        "        if idx not in prediction_dict:\n",
        "            prediction_dict[idx] = {}\n",
        "        prediction_dict[idx][label] = pred\n",
        "\n",
        "    return y_test\n",
        "\n",
        "# Run for all files and store predictions\n",
        "for label, path in file_paths.items():\n",
        "    actual = train_and_predict(path, label)\n",
        "    if actual_marks is None:\n",
        "        actual_marks = actual\n",
        "\n",
        "# Combine into a final comparison table\n",
        "comparison_df = pd.DataFrame.from_dict(prediction_dict, orient=\"index\")\n",
        "comparison_df[\"Y_Actual\"] = actual_marks\n",
        "comparison_df[\"Prediction_Variation\"] = comparison_df.max(axis=1) - comparison_df.min(axis=1)\n",
        "\n",
        "# Sort by variation in predictions\n",
        "sorted_by_variation = comparison_df.sort_values(by=\"Prediction_Variation\", ascending=False)\n",
        "\n",
        "# Pick top 3 variation rows + high, low, mid Y_actual\n",
        "top_variation_rows = sorted_by_variation.head(3)\n",
        "high_score_row = comparison_df[comparison_df[\"Y_Actual\"] == comparison_df[\"Y_Actual\"].max()].head(1)\n",
        "low_score_row = comparison_df[comparison_df[\"Y_Actual\"] == comparison_df[\"Y_Actual\"].min()].head(1)\n",
        "mid_score_row = comparison_df.loc[(comparison_df[\"Y_Actual\"] - comparison_df[\"Y_Actual\"].mean()).abs().argsort()].head(1)\n",
        "\n",
        "# Combine unique rows\n",
        "final_rows = pd.concat([top_variation_rows, high_score_row, low_score_row, mid_score_row]).drop_duplicates()\n",
        "\n",
        "# Final table for analysis\n",
        "print(\"\\n🔍 Selected Rows for Analysis (Top Variation + High/Low/Moderate Marks):\")\n",
        "print(final_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpq7WpNSKOSK",
        "outputId": "2bebf9c1-7784-42a8-b0e0-07c564797cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Selected Rows for Analysis (Top Variation + High/Low/Moderate Marks):\n",
            "   Error Embeddings  Chunked Embeddings  Chunked Logic  Y_Actual  \\\n",
            "1          6.980912            7.096906       7.068449         8   \n",
            "0          7.027394            7.061922       7.055458         8   \n",
            "2          7.035543            7.348223       7.232830         8   \n",
            "\n",
            "   Prediction_Variation  \n",
            "1              1.019088  \n",
            "0              0.972606  \n",
            "2              0.964457  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# File paths\n",
        "file_paths = {\n",
        "    \"Error Embeddings\": \"avg_mark_code_with_error_embeddings.xlsx\",\n",
        "    \"Chunked Embeddings\": \"avg_mark_chunked_code_embeddings.xlsx\",\n",
        "    \"Chunked Logic\": \"avg_Chunked_Embedding_Python_code(logic).xlsx\"\n",
        "}\n",
        "\n",
        "# Store predictions and actuals\n",
        "prediction_dict = {}\n",
        "actual_marks = None\n",
        "\n",
        "# Fast CatBoost run per dataset\n",
        "def run_fast_model(file_path, label):\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    y = df[\"Average Total (Rounded)\"]\n",
        "    X = df.drop(columns=[\"Average Total (Rounded)\"], errors='ignore').select_dtypes(include=np.number)\n",
        "\n",
        "    X_train = X.iloc[5:]\n",
        "    y_train = y.iloc[5:]\n",
        "    X_test = X.iloc[:5]\n",
        "    y_test = y.iloc[:5]\n",
        "\n",
        "    model = CatBoostRegressor(\n",
        "        depth=6,\n",
        "        learning_rate=0.1,\n",
        "        iterations=50,\n",
        "        verbose=0,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    for idx, pred in zip(X_test.index, y_pred):\n",
        "        if idx not in prediction_dict:\n",
        "            prediction_dict[idx] = {}\n",
        "        prediction_dict[idx][label] = pred\n",
        "\n",
        "    return y_test\n",
        "\n",
        "# Run all datasets\n",
        "for label, path in file_paths.items():\n",
        "    actual = run_fast_model(path, label)\n",
        "    if actual_marks is None:\n",
        "        actual_marks = actual\n",
        "\n",
        "# Build final DataFrame\n",
        "comparison_df = pd.DataFrame.from_dict(prediction_dict, orient=\"index\")\n",
        "comparison_df[\"Y_Actual\"] = actual_marks\n",
        "comparison_df[\"Prediction_Variation\"] = comparison_df.max(axis=1) - comparison_df.min(axis=1)\n",
        "\n",
        "# Identify points of interest\n",
        "top_variation = comparison_df.sort_values(by=\"Prediction_Variation\", ascending=False).head(3)\n",
        "high_actual = comparison_df[comparison_df[\"Y_Actual\"] == comparison_df[\"Y_Actual\"].max()].head(1)\n",
        "low_actual = comparison_df[comparison_df[\"Y_Actual\"] == comparison_df[\"Y_Actual\"].min()].head(1)\n",
        "mid_actual = comparison_df.loc[\n",
        "    (comparison_df[\"Y_Actual\"] - comparison_df[\"Y_Actual\"].mean()).abs().argsort()\n",
        "].head(1)\n",
        "\n",
        "# Combine all selected rows\n",
        "final_selection = pd.concat([top_variation, high_actual, low_actual, mid_actual]).drop_duplicates()\n",
        "print(\"\\n🔍 Selected Points (Top Variations + High/Low/Mid Actual):\")\n",
        "print(final_selection)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiBvLIaONzah",
        "outputId": "6fbe1c45-dbca-4c17-df5d-ddf8f8c54178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Selected Points (Top Variations + High/Low/Mid Actual):\n",
            "   Error Embeddings  Chunked Embeddings  Chunked Logic  Y_Actual  \\\n",
            "2          6.930458            7.270196       7.241787         8   \n",
            "0          6.992375            7.124624       7.134850         8   \n",
            "1          6.992922            7.124464       7.121782         8   \n",
            "\n",
            "   Prediction_Variation  \n",
            "2              1.069542  \n",
            "0              1.007625  \n",
            "1              1.007078  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# File paths for your Excel datasets\n",
        "file_paths = {\n",
        "    \"Error Embeddings\": \"avg_mark_code_with_error_embeddings.xlsx\",\n",
        "    \"Chunked Embeddings\": \"avg_mark_chunked_code_embeddings.xlsx\",\n",
        "    \"Chunked Logic\": \"avg_Chunked_Embedding_Python_code(logic).xlsx\"\n",
        "}\n",
        "\n",
        "# Function to train a model and get predictions on first N samples\n",
        "def get_catboost_predictions(file_path, model_name, top_n=30):\n",
        "    df = pd.read_excel(file_path)\n",
        "    target_col = \"Average Total (Rounded)\"\n",
        "\n",
        "    y = df[target_col]\n",
        "    X = df.drop(columns=[target_col], errors='ignore').select_dtypes(include=np.number)\n",
        "\n",
        "    # Optional: Normalize data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split train and test\n",
        "    X_train, X_test = X_scaled[top_n:], X_scaled[:top_n]\n",
        "    y_train, y_test = y[top_n:], y[:top_n]\n",
        "\n",
        "    model = CatBoostRegressor(verbose=0, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"Index\": df.index[:top_n],\n",
        "        \"Y_Actual\": y_test.values,\n",
        "        model_name: preds\n",
        "    })\n",
        "\n",
        "# Get predictions for all datasets\n",
        "results = [get_catboost_predictions(path, name, top_n=30) for name, path in file_paths.items()]\n",
        "\n",
        "# Merge all predictions into one DataFrame\n",
        "merged_df = results[0]\n",
        "for df in results[1:]:\n",
        "    merged_df = pd.merge(merged_df, df, on=[\"Index\", \"Y_Actual\"])\n",
        "\n",
        "# Calculate prediction variation\n",
        "merged_df[\"Prediction_Variation\"] = merged_df[\n",
        "    [\"Error Embeddings\", \"Chunked Embeddings\", \"Chunked Logic\"]\n",
        "].max(axis=1) - merged_df[\n",
        "    [\"Error Embeddings\", \"Chunked Embeddings\", \"Chunked Logic\"]\n",
        "].min(axis=1)\n",
        "\n",
        "# Identify interesting samples\n",
        "top_var = merged_df.sort_values(\"Prediction_Variation\", ascending=False).head(1)\n",
        "high = merged_df.sort_values(\"Y_Actual\", ascending=False).head(1)\n",
        "low = merged_df.sort_values(\"Y_Actual\", ascending=True).head(1)\n",
        "median_actual = merged_df[\"Y_Actual\"].median()\n",
        "mid = merged_df.iloc[(merged_df[\"Y_Actual\"] - median_actual).abs().argsort()[:1]]\n",
        "\n",
        "# Combine and drop duplicates\n",
        "selected_samples = pd.concat([top_var, high, low, mid]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Display selected samples\n",
        "print(\"📌 Selected Points (High/Low/Mid Actual + Max Variation):\")\n",
        "print(selected_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoukPFlqW91u",
        "outputId": "7428c7d1-4766-428e-c88c-c652ffcf2cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Selected Points (High/Low/Mid Actual + Max Variation):\n",
            "   Index  Y_Actual  Error Embeddings  Chunked Embeddings  Chunked Logic  \\\n",
            "0     17         7          7.204461            6.415261       6.355226   \n",
            "1      0         8          7.155315            7.180914       7.169314   \n",
            "2      7         6          6.941093            6.508374       6.421728   \n",
            "3      5         7          7.237700            7.425139       7.514281   \n",
            "\n",
            "   Prediction_Variation  \n",
            "0              0.849236  \n",
            "1              0.025599  \n",
            "2              0.519365  \n",
            "3              0.276581  \n"
          ]
        }
      ]
    }
  ]
}